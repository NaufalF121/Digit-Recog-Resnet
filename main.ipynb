{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Type, Union\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class block(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            intermediate_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            intermediate_channels,\n",
    "            intermediate_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            intermediate_channels,\n",
    "            intermediate_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 784\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            image_channels, 784, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(784)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
    "        self.layer1 = self._make_layer(\n",
    "            block, layers[0], intermediate_channels=64, stride=1\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, layers[1], intermediate_channels=128, stride=2\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, layers[2], intermediate_channels=256, stride=2\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, layers[3], intermediate_channels=512, stride=2\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
    "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
    "        # to the layer that's ahead\n",
    "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    intermediate_channels * 4,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(intermediate_channels * 4),\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
    "        )\n",
    "\n",
    "        # The expansion size is always 4 for ResNet 50,101,152\n",
    "        self.in_channels = intermediate_channels * 4\n",
    "\n",
    "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
    "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
    "        # and also same amount of channels.\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, intermediate_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def ResNet50(img_channel=3, num_classes=1000):\n",
    "    return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)\n",
    "\n",
    "\n",
    "def ResNet101(img_channel=3, num_classes=1000):\n",
    "    return ResNet(block, [3, 4, 23, 3], img_channel, num_classes)\n",
    "\n",
    "\n",
    "def ResNet152(img_channel=3, num_classes=1000):\n",
    "    return ResNet(block, [3, 8, 36, 3], img_channel, num_classes)\n",
    "\n",
    "model = ResNet50(img_channel=1, num_classes=10).to('cpu')\n",
    "PATH = \"model\"\n",
    "model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test,\n",
    "                           batch_size = 100,\n",
    "                           shuffle = True,\n",
    "                           num_workers =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9865\n"
     ]
    }
   ],
   "source": [
    "test_samples_num = 10000\n",
    "correct = 0 \n",
    "\n",
    "model.eval().cpu()\n",
    "\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Make predictions.\n",
    "        prediction = model(inputs)\n",
    "\n",
    "        # Retrieve predictions indexes.\n",
    "        _, predicted_class = torch.max(prediction.data, 1)\n",
    "\n",
    "        # Compute number of correct predictions.\n",
    "        correct += (predicted_class == labels).float().sum().item()\n",
    "\n",
    "test_accuracy = correct / test_samples_num\n",
    "print('Test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaVklEQVR4nO3dX0zV9/3H8dfxD0dt4VBEOFD/FPFfUpVNq4xYXTspyhbjvwu1vbCL0Wixmbr+ic2q7baEzW5N08W0u9I1U9uZTE29MFEsmLVop9U4U8eE0IER0LpxDqKig8/vwl/PdiqoXzyH9wGfj+STyDnfD+ftdyc8dzinX33OOScAAHpYP+sBAAAPJgIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMDLAe4Ns6Ojp04cIFJScny+fzWY8DAPDIOaeWlhZlZ2erX7+uX+ckXIAuXLigESNGWI8BALhP9fX1Gj58eJf3J9yv4JKTk61HAADEwN1+nsctQFu3btVjjz2mQYMGKT8/X59//vk97ePXbgDQN9zt53lcAvTRRx9pw4YN2rx5s7744gvl5eVpzpw5unjxYjweDgDQG7k4mD59uispKYl83d7e7rKzs11paeld94ZCISeJxWKxWL18hUKhO/68j/kroBs3bujEiRMqLCyM3NavXz8VFhaqsrLytuPb2toUDoejFgCg74t5gL7++mu1t7crMzMz6vbMzEw1NjbednxpaakCgUBk8Qk4AHgwmH8KbuPGjQqFQpFVX19vPRIAoAfE/L8DSk9PV//+/dXU1BR1e1NTk4LB4G3H+/1++f3+WI8BAEhwMX8FlJSUpKlTp6qsrCxyW0dHh8rKylRQUBDrhwMA9FJxuRLChg0btHz5cj3xxBOaPn263nnnHbW2turHP/5xPB4OANALxSVAS5Ys0aVLl7Rp0yY1NjbqO9/5jg4cOHDbBxMAAA8un3POWQ/xv8LhsAKBgPUYAID7FAqFlJKS0uX95p+CAwA8mAgQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgImYB+iNN96Qz+eLWhMmTIj1wwAAerkB8fimjz/+uA4dOvTfBxkQl4cBAPRicSnDgAEDFAwG4/GtAQB9RFzeAzp37pyys7M1evRoPffcc6qrq+vy2La2NoXD4agFAOj7Yh6g/Px8bd++XQcOHNB7772n2tpazZw5Uy0tLZ0eX1paqkAgEFkjRoyI9UgAgATkc865eD5Ac3OzRo0apbffflsrVqy47f62tja1tbVFvg6Hw0QIAPqAUCiklJSULu+P+6cDUlNTNW7cOFVXV3d6v9/vl9/vj/cYAIAEE/f/DujKlSuqqalRVlZWvB8KANCLxDxAL730kioqKvTVV1/ps88+08KFC9W/f38tW7Ys1g8FAOjFYv4ruPPnz2vZsmW6fPmyhg0bpieffFJHjx7VsGHDYv1QAIBeLO4fQvAqHA4rEAhYjwEAuE93+xAC14IDAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwOsBwCAviQlJcXznnHjxnnec/z4cc97Eg2vgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE1yMFLhPSUlJnvc888wznvccPnzY855r16553tOT0tLSPO9JTk72vGf8+PGe90jSmjVrPO+ZOXOm5z0tLS2e9+Tk5Hjek2h4BQQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOBipOiTpkyZ0q19M2bM8Lzn2Wef9bxn+vTpnvdUVVV53rN3717Pe7orNzfX854f/OAHnvcMHTrU8x7nnOc93dXa2up5z6ZNm+IwSeLjFRAAwAQBAgCY8BygI0eOaN68ecrOzpbP57vtJb5zTps2bVJWVpYGDx6swsJCnTt3LlbzAgD6CM8Bam1tVV5enrZu3drp/Vu2bNG7776r999/X8eOHdNDDz2kOXPm6Pr16/c9LACg7/D8IYTi4mIVFxd3ep9zTu+8845+9rOfaf78+ZKkDz74QJmZmdq7d6+WLl16f9MCAPqMmL4HVFtbq8bGRhUWFkZuCwQCys/PV2VlZad72traFA6HoxYAoO+LaYAaGxslSZmZmVG3Z2ZmRu77ttLSUgUCgcgaMWJELEcCACQo80/Bbdy4UaFQKLLq6+utRwIA9ICYBigYDEqSmpqaom5vamqK3Pdtfr9fKSkpUQsA0PfFNEA5OTkKBoMqKyuL3BYOh3Xs2DEVFBTE8qEAAL2c50/BXblyRdXV1ZGva2trderUKaWlpWnkyJFat26dfvnLX2rs2LHKycnR66+/ruzsbC1YsCCWcwMAejnPATp+/LiefvrpyNcbNmyQJC1fvlzbt2/XK6+8otbWVq1atUrNzc168skndeDAAQ0aNCh2UwMAej2f68mr9N2DcDisQCBgPQYSyNSpUz3vOXr0aLceq1+/nvlczl//+lfPe9LS0jzv6c4FQnvSf/7zH897zp4963nP3/72N897JOngwYOe95SXl3veU1dX53lPbxAKhe74vr75p+AAAA8mAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmPD8zzEAPW3s2LGe93T3qtbt7e2e97z22mue9+zYscPznn/961+e9xQVFXneI0lPPPGE5z3duXJ0d/5OX375pec9SEy8AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPicc856iP8VDocVCASsx0CcTJw40fOeiooKz3tSU1M975GkpUuXet6ze/fubj0W0NeFQiGlpKR0eT+vgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwOsB8CD5dKlS573pKWled7T3WvsTpkyxfMev9/vec+RI0c87/H5fJ73dPc8nD9/3vOejo6Obj0WHly8AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPhcd69WGCfhcFiBQMB6DCSQf/zjH5735ObmxmESWz15MdJXX33V857f/OY33Xos9F2hUEgpKSld3s8rIACACQIEADDhOUBHjhzRvHnzlJ2dLZ/Pp71790bd//zzz8vn80WtuXPnxmpeAEAf4TlAra2tysvL09atW7s8Zu7cuWpoaIisXbt23deQAIC+x/O/iFpcXKzi4uI7HuP3+xUMBrs9FACg74vLe0Dl5eXKyMjQ+PHjtWbNGl2+fLnLY9va2hQOh6MWAKDvi3mA5s6dqw8++EBlZWX69a9/rYqKChUXF6u9vb3T40tLSxUIBCJrxIgRsR4JAJCAPP8K7m6WLl0a+fOkSZM0efJk5ebmqry8XLNnz77t+I0bN2rDhg2Rr8PhMBECgAdA3D+GPXr0aKWnp6u6urrT+/1+v1JSUqIWAKDvi3uAzp8/r8uXLysrKyveDwUA6EU8/wruypUrUa9mamtrderUKaWlpSktLU1vvvmmFi9erGAwqJqaGr3yyisaM2aM5syZE9PBAQC9m+cAHT9+XE8//XTk62/ev1m+fLnee+89nT59Wn/4wx/U3Nys7OxsFRUV6Re/+IX8fn/spgYA9HpcjDSBLVu2zPOeQ4cOed5z6dIlz3t6Unf+m7K33norDpN07plnnvG8Z9iwYZ739OTFSIuKijzvKSsr69Zjoe/iYqQAgIREgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEzH/J7kROzt27PC856uvvvK85/z585739KSevAp0d/TUPzVy48YNz3v27t3brcf69NNPu7UP8IJXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACS5GmsC++93vet5TUVHhec+oUaM87+lJiX4x0u44fPiw5z0HDhzwvOe3v/2t5z1AT+EVEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgwucS7KqN4XBYgUDAegzEyezZsz3vGTRokOc9//73vz3vkaTPPvusW/sA3C4UCiklJaXL+3kFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GKkAIC44GKkAICERIAAACY8Bai0tFTTpk1TcnKyMjIytGDBAlVVVUUdc/36dZWUlGjo0KF6+OGHtXjxYjU1NcV0aABA7+cpQBUVFSopKdHRo0d18OBB3bx5U0VFRWptbY0cs379en388cfavXu3KioqdOHCBS1atCjmgwMAejl3Hy5evOgkuYqKCuecc83NzW7gwIFu9+7dkWPOnj3rJLnKysp7+p6hUMhJYrFYLFYvX6FQ6I4/7+/rPaBQKCRJSktLkySdOHFCN2/eVGFhYeSYCRMmaOTIkaqsrOz0e7S1tSkcDkctAEDf1+0AdXR0aN26dZoxY4YmTpwoSWpsbFRSUpJSU1Ojjs3MzFRjY2On36e0tFSBQCCyRowY0d2RAAC9SLcDVFJSojNnzujDDz+8rwE2btyoUCgUWfX19ff1/QAAvcOA7mxau3at9u/fryNHjmj48OGR24PBoG7cuKHm5uaoV0FNTU0KBoOdfi+/3y+/39+dMQAAvZinV0DOOa1du1Z79uzR4cOHlZOTE3X/1KlTNXDgQJWVlUVuq6qqUl1dnQoKCmIzMQCgT/D0CqikpEQ7d+7Uvn37lJycHHlfJxAIaPDgwQoEAlqxYoU2bNigtLQ0paSk6MUXX1RBQYG+973vxeUvAADopbx87FpdfNRu27ZtkWOuXbvmXnjhBffII4+4IUOGuIULF7qGhoZ7fgw+hs1isVh9Y93tY9hcjBQAEBdcjBQAkJAIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJTwEqLS3VtGnTlJycrIyMDC1YsEBVVVVRxzz11FPy+XxRa/Xq1TEdGgDQ+3kKUEVFhUpKSnT06FEdPHhQN2/eVFFRkVpbW6OOW7lypRoaGiJry5YtMR0aAND7DfBy8IEDB6K+3r59uzIyMnTixAnNmjUrcvuQIUMUDAZjMyEAoE+6r/eAQqGQJCktLS3q9h07dig9PV0TJ07Uxo0bdfXq1S6/R1tbm8LhcNQCADwAXDe1t7e7H/3oR27GjBlRt//+9793Bw4ccKdPn3Z//OMf3aOPPuoWLlzY5ffZvHmzk8RisVisPrZCodAdO9LtAK1evdqNGjXK1dfX3/G4srIyJ8lVV1d3ev/169ddKBSKrPr6evOTxmKxWKz7X3cLkKf3gL6xdu1a7d+/X0eOHNHw4cPveGx+fr4kqbq6Wrm5ubfd7/f75ff7uzMGAKAX8xQg55xefPFF7dmzR+Xl5crJybnrnlOnTkmSsrKyujUgAKBv8hSgkpIS7dy5U/v27VNycrIaGxslSYFAQIMHD1ZNTY127typH/7whxo6dKhOnz6t9evXa9asWZo8eXJc/gIAgF7Ky/s+6uL3fNu2bXPOOVdXV+dmzZrl0tLSnN/vd2PGjHEvv/zyXX8P+L9CoZD57y1ZLBaLdf/rbj/7ff8floQRDocVCASsxwAA3KdQKKSUlJQu7+dacAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwkXIOec9QgAgBi428/zhAtQS0uL9QgAgBi4289zn0uwlxwdHR26cOGCkpOT5fP5ou4Lh8MaMWKE6uvrlZKSYjShPc7DLZyHWzgPt3AebkmE8+CcU0tLi7Kzs9WvX9evcwb04Ez3pF+/fho+fPgdj0lJSXmgn2Df4Dzcwnm4hfNwC+fhFuvzEAgE7npMwv0KDgDwYCBAAAATvSpAfr9fmzdvlt/vtx7FFOfhFs7DLZyHWzgPt/Sm85BwH0IAADwYetUrIABA30GAAAAmCBAAwAQBAgCY6DUB2rp1qx577DENGjRI+fn5+vzzz61H6nFvvPGGfD5f1JowYYL1WHF35MgRzZs3T9nZ2fL5fNq7d2/U/c45bdq0SVlZWRo8eLAKCwt17tw5m2Hj6G7n4fnnn7/t+TF37lybYeOktLRU06ZNU3JysjIyMrRgwQJVVVVFHXP9+nWVlJRo6NChevjhh7V48WI1NTUZTRwf93IennrqqdueD6tXrzaauHO9IkAfffSRNmzYoM2bN+uLL75QXl6e5syZo4sXL1qP1uMef/xxNTQ0RNZf/vIX65HirrW1VXl5edq6dWun92/ZskXvvvuu3n//fR07dkwPPfSQ5syZo+vXr/fwpPF1t/MgSXPnzo16fuzatasHJ4y/iooKlZSU6OjRozp48KBu3rypoqIitba2Ro5Zv369Pv74Y+3evVsVFRW6cOGCFi1aZDh17N3LeZCklStXRj0ftmzZYjRxF1wvMH36dFdSUhL5ur293WVnZ7vS0lLDqXre5s2bXV5envUYpiS5PXv2RL7u6OhwwWDQvfXWW5Hbmpubnd/vd7t27TKYsGd8+zw459zy5cvd/PnzTeaxcvHiRSfJVVRUOOdu/W8/cOBAt3v37sgxZ8+edZJcZWWl1Zhx9+3z4Jxz3//+991PfvITu6HuQcK/Arpx44ZOnDihwsLCyG39+vVTYWGhKisrDSezce7cOWVnZ2v06NF67rnnVFdXZz2SqdraWjU2NkY9PwKBgPLz8x/I50d5ebkyMjI0fvx4rVmzRpcvX7YeKa5CoZAkKS0tTZJ04sQJ3bx5M+r5MGHCBI0cObJPPx++fR6+sWPHDqWnp2vixInauHGjrl69ajFelxLuYqTf9vXXX6u9vV2ZmZlRt2dmZurvf/+70VQ28vPztX37do0fP14NDQ168803NXPmTJ05c0bJycnW45lobGyUpE6fH9/c96CYO3euFi1apJycHNXU1Oi1115TcXGxKisr1b9/f+vxYq6jo0Pr1q3TjBkzNHHiREm3ng9JSUlKTU2NOrYvPx86Ow+S9Oyzz2rUqFHKzs7W6dOn9eqrr6qqqkp//vOfDaeNlvABwn8VFxdH/jx58mTl5+dr1KhR+tOf/qQVK1YYToZEsHTp0sifJ02apMmTJys3N1fl5eWaPXu24WTxUVJSojNnzjwQ74PeSVfnYdWqVZE/T5o0SVlZWZo9e7ZqamqUm5vb02N2KuF/BZeenq7+/fvf9imWpqYmBYNBo6kSQ2pqqsaNG6fq6mrrUcx88xzg+XG70aNHKz09vU8+P9auXav9+/frk08+ifrnW4LBoG7cuKHm5uao4/vq86Gr89CZ/Px8SUqo50PCBygpKUlTp05VWVlZ5LaOjg6VlZWpoKDAcDJ7V65cUU1NjbKysqxHMZOTk6NgMBj1/AiHwzp27NgD//w4f/68Ll++3KeeH845rV27Vnv27NHhw4eVk5MTdf/UqVM1cODAqOdDVVWV6urq+tTz4W7noTOnTp2SpMR6Plh/CuJefPjhh87v97vt27e7L7/80q1atcqlpqa6xsZG69F61E9/+lNXXl7uamtr3aeffuoKCwtdenq6u3jxovVocdXS0uJOnjzpTp486SS5t99+2508edL985//dM4596tf/cqlpqa6ffv2udOnT7v58+e7nJwcd+3aNePJY+tO56GlpcW99NJLrrKy0tXW1rpDhw65KVOmuLFjx7rr169bjx4za9ascYFAwJWXl7uGhobIunr1auSY1atXu5EjR7rDhw+748ePu4KCAldQUGA4dezd7TxUV1e7n//85+748eOutrbW7du3z40ePdrNmjXLePJovSJAzjn3u9/9zo0cOdIlJSW56dOnu6NHj1qP1OOWLFnisrKyXFJSknv00UfdkiVLXHV1tfVYcffJJ584Sbet5cuXO+dufRT79ddfd5mZmc7v97vZs2e7qqoq26Hj4E7n4erVq66oqMgNGzbMDRw40I0aNcqtXLmyz/2ftM7+/pLctm3bIsdcu3bNvfDCC+6RRx5xQ4YMcQsXLnQNDQ12Q8fB3c5DXV2dmzVrlktLS3N+v9+NGTPGvfzyyy4UCtkO/i38cwwAABMJ/x4QAKBvIkAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBM/B+hzIKP1LXNHwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "st.title(\"Resnet with MNIST\")\n",
    "\n",
    "def show():\n",
    "    sample_idx = torch.randint(len(test), size=(1,)).item()\n",
    "    print(f\"Index gambar : {sample_idx}\")\n",
    "    image, label = test[sample_idx]\n",
    "    # plt.imshow(image.squeeze(), cmap = 'gray')\n",
    "    # plt.show()\n",
    "    return sample_idx\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
